from skorch import NeuralNet
from skorch.utils import to_device
from skorch.utils import to_tensor
from services.layers import *
from skorch.dataset import unpack_data


# skorch objects
class DCSRO_Net(NeuralNet):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def get_loss(self, y_pred, y_true, X=None, training=False):
        loss = super().get_loss(y_pred, y_true, X=X, training=training)
        return loss.mean()

    def score(self, X, y=None):
        y_pred = self.forward(to_tensor(X, self.device))
        # ipdb.set_trace()
        loss = super().get_loss(to_device(y_pred, self.device),
                                to_tensor(y, self.device),
                                X=to_tensor(X, self.device),
                                training=False)
        loss_value = loss.mean()
        return -loss_value


class Static_PCRSO(NeuralNet):

    def __init__(self, *args, lambda_=1, **kwargs):
        super().__init__(*args, **kwargs)
        self.lambda_ = lambda_

    def initialize_module(self):
        super().initialize_module()

        # add an additional module called 'module2_'
        params = self.get_params_for('loss_approximator')
        self.loss_approximator_ = loss_module(**params)
        return self

    def initialize_optimizer(self):
        # first initialize the normal optimizer
        named_params = self.module_.named_parameters()
        args, kwargs = self.get_params_for_optimizer('optimizer', named_params)
        self.optimizer_ = self.optimizer(*args, **kwargs)
        return self

    def train_step(self, batch, **fit_params):
        """Prepares a loss function callable and pass it to the optimizer,
        hence performing one optimization step.

        Loss function callable as required by some optimizers (and accepted by
        all of them):
        https://pytorch.org/docs/master/optim.html#optimizer-step-closure

        The module is set to be in train mode (e.g. dropout is
        applied).

        Parameters
        ----------
        batch
          A single batch returned by the data loader.

        **fit_params : dict
          Additional parameters passed to the ``forward`` method of
          the module and to the train_split call.

        Returns
        -------
        step : dict
          A dictionary ``{'loss': loss, 'y_pred': y_pred}``, where the
          float ``loss`` is the result of the loss function and
          ``y_pred`` the prediction generated by the PyTorch module.

        """

        epoch_num = len(self.history[:, ])
        step_accumulator = self.get_train_step_accumulator()
        self._set_training(True)

        # update the task
        self.optimizer_.zero_grad()
        Xi, y_true = unpack_data(batch)
        y_pred = self.infer(Xi, **fit_params)
        loss = self.get_loss(y_pred, y_true, X=Xi, training=True).mean()  # MMD loss
        # print(y_true.shape)
        batch_size, _, n_series = y_true.shape
        n_series = int(n_series / 2)

        y_true = to_device(y_true, self.device)
        # y_pred = torch.unsqueeze(y_pred, 0) #K by 2n tensor add a dim to the first spot

        # y_pred = torch.repeat_interleave(y_pred, batch_size, dim = 0)
        # y_true = torch.unsqueeze(y_true, -2)

        stacked_scenarios = torch.concat((y_pred, y_true), axis=-2)
        # print(stacked_scenarios.shape)
        loss_prediction_error = 0
        task_loss = 0

        task_loss = -1 * self.lambda_ * self.loss_approximator_(stacked_scenarios).mean()

        loss += task_loss

        loss.backward()
        step = {'loss': loss, 'y_pred': y_pred, }  # store MMD loss
        step_accumulator.store_step(step)
        self.optimizer_.step()

        self.history.record_batch('Task Loss', task_loss.cpu().detach().numpy() / self.lambda_)

        return step_accumulator.get_step()
